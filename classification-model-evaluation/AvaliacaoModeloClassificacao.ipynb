{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQumGNuKjVXk",
        "outputId": "8bb82fe2-e73c-4685-a162-375faab20978"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM (C=0.01):\n",
            "  Acurácia: 0.9983\n",
            "  Precisão: 0.9983\n",
            "  Recall: 0.9983\n",
            "  F1-score: 0.9983\n",
            "  ROC-AUC: N/A\n",
            "SVM (C=0.1):\n",
            "  Acurácia: 0.9983\n",
            "  Precisão: 0.9983\n",
            "  Recall: 0.9983\n",
            "  F1-score: 0.9983\n",
            "  ROC-AUC: N/A\n",
            "SVM (C=1):\n",
            "  Acurácia: 0.9983\n",
            "  Precisão: 0.9983\n",
            "  Recall: 0.9983\n",
            "  F1-score: 0.9983\n",
            "  ROC-AUC: N/A\n",
            "SVM (C=10):\n",
            "  Acurácia: 0.9983\n",
            "  Precisão: 0.9983\n",
            "  Recall: 0.9983\n",
            "  F1-score: 0.9983\n",
            "  ROC-AUC: N/A\n",
            "SVM (C=100):\n",
            "  Acurácia: 0.9983\n",
            "  Precisão: 0.9983\n",
            "  Recall: 0.9983\n",
            "  F1-score: 0.9983\n",
            "  ROC-AUC: N/A\n",
            "\n",
            "KNN (k=1):\n",
            "  Acurácia: 0.9983\n",
            "  Precisão: 0.9983\n",
            "  Recall: 0.9983\n",
            "  F1-score: 0.9983\n",
            "  ROC-AUC: 0.9946236559139785\n",
            "\n",
            "KNN (k=3):\n",
            "  Acurácia: 0.9979\n",
            "  Precisão: 0.9979\n",
            "  Recall: 0.9979\n",
            "  F1-score: 0.9979\n",
            "  ROC-AUC: 0.9999861076380206\n",
            "\n",
            "KNN (k=5):\n",
            "  Acurácia: 0.9967\n",
            "  Precisão: 0.9967\n",
            "  Recall: 0.9967\n",
            "  F1-score: 0.9967\n",
            "  ROC-AUC: 0.9999305381901031\n",
            "\n",
            "KNN (k=7):\n",
            "  Acurácia: 0.9946\n",
            "  Precisão: 0.9946\n",
            "  Recall: 0.9946\n",
            "  F1-score: 0.9945\n",
            "  ROC-AUC: 0.9999861076380205\n",
            "\n",
            "KNN (k=9):\n",
            "  Acurácia: 0.9933\n",
            "  Precisão: 0.9934\n",
            "  Recall: 0.9933\n",
            "  F1-score: 0.9933\n",
            "  ROC-AUC: 0.9999722152760412\n",
            "\n",
            "Melhor C para SVM: 0.01\n",
            "Melhor k para KNN: 1\n",
            "\n",
            "Métricas finais - SVM:\n",
            "  Acurácia: 0.2000\n",
            "  Precisão: 0.0400\n",
            "  Recall: 0.2000\n",
            "  F1-score: 0.0667\n",
            "  ROC-AUC: N/A\n",
            "\n",
            "Métricas finais - KNN:\n",
            "  Acurácia: 0.8000\n",
            "  Precisão: 0.6400\n",
            "  Recall: 0.8000\n",
            "  F1-score: 0.7111\n",
            "  ROC-AUC: 0.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    roc_auc_score,\n",
        ")\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "\n",
        "\n",
        "def load_data(train_filepath=None, test_filepath=None):\n",
        "    \"\"\"\n",
        "    Carrega dados de treino e teste de diferentes formatos (.npz, .csv, .json, .xlsx, etc.).\n",
        "\n",
        "    Args:\n",
        "        train_filepath (str): Caminho do arquivo de treinamento.\n",
        "        test_filepath (str): Caminho do arquivo de teste.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (X_train, y_train, X_test, y_test)\n",
        "    \"\"\"\n",
        "\n",
        "    def _load_file(filepath):\n",
        "        \"\"\"Carrega um único arquivo com base na extensão.\"\"\"\n",
        "        if filepath is None:\n",
        "            return None, None\n",
        "\n",
        "        file_extension = Path(filepath).suffix.lower()\n",
        "\n",
        "        if file_extension == \".npz\":\n",
        "            data = np.load(filepath)\n",
        "            return data[\"features\"], data[\"labels\"]\n",
        "        elif file_extension == \".csv\":\n",
        "            df = pd.read_csv(filepath)\n",
        "            return df.iloc[:, :-1].values, df.iloc[:, -1].values\n",
        "        elif file_extension == \".xlsx\":\n",
        "            df = pd.read_excel(filepath)\n",
        "            return df.iloc[:, :-1].values, df.iloc[:, -1].values\n",
        "        elif file_extension == \".json\":\n",
        "            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
        "                data = json.load(f)\n",
        "            return np.array(data[\"features\"]), np.array(data[\"labels\"])\n",
        "        elif file_extension == \".parquet\":\n",
        "            df = pd.read_parquet(filepath)\n",
        "            return df.iloc[:, :-1].values, df.iloc[:, -1].values\n",
        "        else:\n",
        "            raise ValueError(f\"Formato de arquivo não suportado: {file_extension}\")\n",
        "\n",
        "    X_train, y_train = _load_file(train_filepath)\n",
        "    X_test, y_test = _load_file(test_filepath)\n",
        "\n",
        "    if X_train is None or y_train is None:\n",
        "        raise ValueError(\"Arquivo de treino não foi carregado corretamente.\")\n",
        "    if X_test is None or y_test is None:\n",
        "        raise ValueError(\"Arquivo de teste não foi carregado corretamente.\")\n",
        "\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "\n",
        "def load_data2(filepath=None):\n",
        "          \"\"\"\n",
        "          Carrega dados de diferentes formatos, como .npz, .csv, .json, .xlsx, etc.\n",
        "\n",
        "          Args:\n",
        "              filepath (str): Caminho do arquivo. Se None, carrega o dataset Iris.\n",
        "\n",
        "          Returns:\n",
        "              tuple: (features, labels) onde features são as features e labels são os rótulos.\n",
        "          \"\"\"\n",
        "          if filepath is None:\n",
        "              dataset = load_iris()\n",
        "              return dataset.data, dataset.target\n",
        "\n",
        "          file_extension = Path(filepath).suffix.lower()\n",
        "\n",
        "          if file_extension == \".npz\":\n",
        "              data = np.load(filepath)\n",
        "              X, y = data[\"features\"], data[\"labels\"]\n",
        "          elif file_extension == \".csv\":\n",
        "              df = pd.read_csv(filepath)\n",
        "              X, y = df.iloc[:, :-1].values, df.iloc[:, -1].values\n",
        "          elif file_extension == \".xlsx\":\n",
        "              df = pd.read_excel(filepath)\n",
        "              X, y = df.iloc[:, :-1].values, df.iloc[:, -1].values\n",
        "          elif file_extension == \".json\":\n",
        "              with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
        "                  data = json.load(f)\n",
        "              X, y = np.array(data[\"features\"]), np.array(data[\"labels\"])\n",
        "          elif file_extension == \".parquet\":\n",
        "              df = pd.read_parquet(filepath)\n",
        "              X, y = df.iloc[:, :-1].values, df.iloc[:, -1].values\n",
        "          else:\n",
        "              raise ValueError(f\"Formato de arquivo não suportado: {file_extension}\")\n",
        "\n",
        "          return X, y\n",
        "\n",
        "class CrossValidation:\n",
        "    \"\"\"\n",
        "    Classe para realizar validação cruzada e avaliar modelos.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_splits=5, random_state=42):\n",
        "        self.n_splits = n_splits\n",
        "        self.random_state = random_state\n",
        "        self.kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
        "\n",
        "    def evaluate_model(self, model, X, y):\n",
        "        \"\"\"\n",
        "        Avalia um modelo usando validação cruzada.\n",
        "        Args:\n",
        "            model: Instância do modelo a ser avaliado.\n",
        "            X (np.ndarray): Dados de entrada.\n",
        "            y (np.ndarray): Rótulos.\n",
        "        Returns:\n",
        "            dict: Dicionário com as métricas de avaliação (acurácia, precisão, recall, F1-score, ROC-AUC).\n",
        "        \"\"\"\n",
        "        accuracies = []\n",
        "        precisions = []\n",
        "        recalls = []\n",
        "        f1_scores = []\n",
        "        roc_aucs = []\n",
        "\n",
        "        for train_index, val_index in self.kf.split(X, y):\n",
        "            X_train, X_val = X[train_index], X[val_index]\n",
        "            y_train, y_val = y[train_index], y[val_index]\n",
        "\n",
        "            model.fit(X_train, y_train)\n",
        "            y_pred = model.predict(X_val)\n",
        "            y_pred_proba = model.predict_proba(X_val) if hasattr(model, \"predict_proba\") else None\n",
        "\n",
        "            # Calcular métricas\n",
        "            accuracies.append(accuracy_score(y_val, y_pred))\n",
        "            precisions.append(precision_score(y_val, y_pred, average=\"weighted\"))\n",
        "            recalls.append(recall_score(y_val, y_pred, average=\"weighted\"))\n",
        "            f1_scores.append(f1_score(y_val, y_pred, average=\"weighted\"))\n",
        "\n",
        "            # ROC-AUC (apenas para modelos que suportam predict_proba)\n",
        "        if y_pred_proba is not None:\n",
        "            # Verifica se é binário ou multiclasse\n",
        "            if y_pred_proba.shape[1] == 2:  # Problema binário\n",
        "                roc_aucs.append(roc_auc_score(y_val, y_pred_proba[:, 1]))\n",
        "            else:  # Problema multiclasse\n",
        "                y_val_bin = label_binarize(y_val, classes=np.unique(y))\n",
        "                roc_aucs.append(roc_auc_score(y_val_bin, y_pred_proba, multi_class=\"ovr\"))\n",
        "\n",
        "        # Retornar métricas médias\n",
        "        metrics = {\n",
        "            \"accuracy\": np.mean(accuracies),\n",
        "            \"precision\": np.mean(precisions),\n",
        "            \"recall\": np.mean(recalls),\n",
        "            \"f1_score\": np.mean(f1_scores),\n",
        "            \"roc_auc\": np.mean(roc_aucs) if roc_aucs else None,\n",
        "        }\n",
        "        return metrics\n",
        "\n",
        "\n",
        "class ModelTrainer:\n",
        "    \"\"\"\n",
        "    Classe para treinar e avaliar modelos com base nos melhores hiperparâmetros.\n",
        "    \"\"\"\n",
        "    def __init__(self, model, best_params):\n",
        "        self.model = model\n",
        "        self.best_params = best_params\n",
        "\n",
        "    def train_final_model(self, X_train, y_train):\n",
        "        \"\"\"\n",
        "        Treina o modelo final com os melhores hiperparâmetros.\n",
        "        Args:\n",
        "            X_train (np.ndarray): Dados de treinamento.\n",
        "            y_train (np.ndarray): Rótulos de treinamento.\n",
        "        \"\"\"\n",
        "        self.model.set_params(**self.best_params)\n",
        "        self.model.fit(X_train, y_train)\n",
        "\n",
        "    def evaluate_final_model(self, X_test, y_test):\n",
        "        \"\"\"\n",
        "        Avalia o modelo final no conjunto de teste.\n",
        "        Args:\n",
        "            X_test (np.ndarray): Dados de teste.\n",
        "            y_test (np.ndarray): Rótulos de teste.\n",
        "        Returns:\n",
        "            dict: Dicionário com as métricas de avaliação (acurácia, precisão, recall, F1-score, ROC-AUC).\n",
        "        \"\"\"\n",
        "        y_pred = self.model.predict(X_test)\n",
        "        y_pred_proba = self.model.predict_proba(X_test) if hasattr(self.model, \"predict_proba\") else None\n",
        "\n",
        "        # ROC-AUC (apenas para modelos que suportam predict_proba)\n",
        "        if y_pred_proba is not None:\n",
        "            # Verifica se é binário ou multiclasse\n",
        "            if y_pred_proba.shape[1] == 2:  # Problema binário\n",
        "                roc_aucs = roc_auc_score(y_test, y_pred_proba[:, 1])\n",
        "            else:  # Problema multiclasse\n",
        "                #y_val_bin = label_binarize(y_test, classes=np.unique(y))\n",
        "                #roc_aucs = roc_auc_score(y_val_bin, y_pred_proba, multi_class=\"ovr\")\n",
        "                roc_aucs = None\n",
        "\n",
        "        # Calcular métricas\n",
        "        metrics = {\n",
        "            \"accuracy\": accuracy_score(y_test, y_pred),\n",
        "            \"precision\": precision_score(y_test, y_pred, average=\"weighted\"),\n",
        "            \"recall\": recall_score(y_test, y_pred, average=\"weighted\"),\n",
        "            \"f1_score\": f1_score(y_test, y_pred, average=\"weighted\"),\n",
        "            \"roc_auc\": roc_aucs if y_pred_proba is not None else None,\n",
        "        }\n",
        "        return metrics\n",
        "\n",
        "\n",
        "class Experiment:\n",
        "    \"\"\"\n",
        "    Classe para executar o experimento completo.\n",
        "    \"\"\"\n",
        "    def __init__(self, train_filepath, test_filepath,random_state=42):\n",
        "        self.X_train_val, self.y_train_val, self.X_test, self.y_test = load_data(train_filepath, test_filepath)\n",
        "        self.random_state = random_state\n",
        "\n",
        "\n",
        "    def _split_data(self):\n",
        "        \"\"\"\n",
        "        Divide o dataset em treino/validação e teste.\n",
        "        Returns:\n",
        "            tuple: (X_train_val, X_test, y_train_val, y_test)\n",
        "        \"\"\"\n",
        "        return train_test_split(self.X, self.y, test_size=self.test_size, random_state=self.random_state)\n",
        "\n",
        "\n",
        "    def run_experiment(self):\n",
        "        \"\"\"\n",
        "        Executa o experimento completo.\n",
        "        \"\"\"\n",
        "        # Configurações de hiperparâmetros\n",
        "        svm_params = {'C': [0.01, 0.1, 1, 10, 100]}\n",
        "        knn_params = {'n_neighbors': [1, 3, 5, 7, 9]}\n",
        "\n",
        "        # Validação cruzada para SVM\n",
        "        cv = CrossValidation()\n",
        "        svm_results = {}\n",
        "        for C in svm_params['C']:\n",
        "            model = LinearSVC(random_state=self.random_state)\n",
        "            metrics = cv.evaluate_model(model.set_params(C=C), self.X_train_val, self.y_train_val)\n",
        "            svm_results[C] = metrics\n",
        "            print(f\"SVM (C={C}):\")\n",
        "            print(f\"  Acurácia: {metrics['accuracy']:.4f}\")\n",
        "            print(f\"  Precisão: {metrics['precision']:.4f}\")\n",
        "            print(f\"  Recall: {metrics['recall']:.4f}\")\n",
        "            print(f\"  F1-score: {metrics['f1_score']:.4f}\")\n",
        "            print(f\"  ROC-AUC: {metrics['roc_auc'] if metrics['roc_auc'] is not None else 'N/A'}\")\n",
        "\n",
        "        # Validação cruzada para KNN\n",
        "        knn_results = {}\n",
        "        for k in knn_params['n_neighbors']:\n",
        "            model = KNeighborsClassifier()\n",
        "            metrics = cv.evaluate_model(model.set_params(n_neighbors=k), self.X_train_val, self.y_train_val)\n",
        "            knn_results[k] = metrics\n",
        "            print(f\"\\nKNN (k={k}):\")\n",
        "            print(f\"  Acurácia: {metrics['accuracy']:.4f}\")\n",
        "            print(f\"  Precisão: {metrics['precision']:.4f}\")\n",
        "            print(f\"  Recall: {metrics['recall']:.4f}\")\n",
        "            print(f\"  F1-score: {metrics['f1_score']:.4f}\")\n",
        "            print(f\"  ROC-AUC: {metrics['roc_auc'] if metrics['roc_auc'] is not None else 'N/A'}\")\n",
        "\n",
        "        # Selecionar os melhores hiperparâmetros\n",
        "        best_C = max(svm_results, key=lambda x: svm_results[x]['accuracy'])\n",
        "        best_k = max(knn_results, key=lambda x: knn_results[x]['accuracy'])\n",
        "        print(f\"\\nMelhor C para SVM: {best_C}\")\n",
        "        print(f\"Melhor k para KNN: {best_k}\")\n",
        "\n",
        "        # Treinar e avaliar os modelos finais\n",
        "        svm_trainer = ModelTrainer(LinearSVC(random_state=self.random_state), {'C': best_C})\n",
        "        svm_trainer.train_final_model(self.X_train_val, self.y_train_val)\n",
        "        svm_metrics = svm_trainer.evaluate_final_model(self.X_test, self.y_test)\n",
        "        print(\"\\nMétricas finais - SVM:\")\n",
        "        print(f\"  Acurácia: {svm_metrics['accuracy']:.4f}\")\n",
        "        print(f\"  Precisão: {svm_metrics['precision']:.4f}\")\n",
        "        print(f\"  Recall: {svm_metrics['recall']:.4f}\")\n",
        "        print(f\"  F1-score: {svm_metrics['f1_score']:.4f}\")\n",
        "        print(f\"  ROC-AUC: {svm_metrics['roc_auc'] if svm_metrics['roc_auc'] is not None else 'N/A'}\")\n",
        "\n",
        "        knn_trainer = ModelTrainer(KNeighborsClassifier(), {'n_neighbors': best_k})\n",
        "        knn_trainer.train_final_model(self.X_train_val, self.y_train_val)\n",
        "        knn_metrics = knn_trainer.evaluate_final_model(self.X_test, self.y_test)\n",
        "        print(\"\\nMétricas finais - KNN:\")\n",
        "        print(f\"  Acurácia: {knn_metrics['accuracy']:.4f}\")\n",
        "        print(f\"  Precisão: {knn_metrics['precision']:.4f}\")\n",
        "        print(f\"  Recall: {knn_metrics['recall']:.4f}\")\n",
        "        print(f\"  F1-score: {knn_metrics['f1_score']:.4f}\")\n",
        "        print(f\"  ROC-AUC: {knn_metrics['roc_auc'] if knn_metrics['roc_auc'] is not None else 'N/A'}\")\n",
        "\n",
        "\n",
        "# Executar o experimento\n",
        "if __name__ == \"__main__\":\n",
        "    #iris = load_iris()\n",
        "    train_file = \"/content/drive/My Drive/Mestrado 2024/Projetos/VIT and CNN/Resultados/Dados compilados/google_vit_large_patch32_384.npz\"  # caminho do seu arquivo\n",
        "    test_file = \"/content/drive/My Drive/Mestrado 2024/Projetos/VIT and CNN/Resultados/Dados compilados/google_vit_large_patch32_384_test.npz\"  # caminho do seu arquivo\n",
        "\n",
        "    experiment = Experiment(train_file, test_file)\n",
        "\n",
        "    experiment.run_experiment()"
      ]
    }
  ]
}